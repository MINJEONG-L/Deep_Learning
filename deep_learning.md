# deep learning  



w 값은 똑같이 쓰는데 
alpha 는 하이퍼 파라미터 ==> learning rate오 ㅏ다름
즉 제약의 크기라고 ㅎㄹ 수 잇음

전첵 작아지려면 alpha 값을 키우면 w는 0에 수렴할 수 밖ㅇ없음
어느정도 가다가 멈춘거나 마찬가지 실제로는 ㅇ
그러나 w가 0에 수렴한채로.. 멈축거랑 비슷
alpha 가 0이되면 규제가 없는것과 마찬가지 
그냥 rss가 최소가 되는 점을 찾는것과같음



다항회귀에서 1-10차수까지 늘리면서 학습
1차수 일때 람다를 0-5까지 바꾸면서 하이퍼파라ㅣㅁ터 ㄴ튜닝
: 편향 분산을 줄여나가면서 

w값이 주어들면서 전체적으로 평평하게.... 윙래로 움직이는 값이 줄어드들음 
 
 w가 크다는 것은 결과값에 많은 영향  
 
알팓값을 높이니까 ㄴnox 값의 순서가 바뀌었따. ==> 
nox 데이터가 잘못들어왓다
아웃라이어가 많다거나 편향된 데잍거ㅏ 많다거나
다른 계쑤와 상관관계...가 잘못잡혓거나알
nox 값이 의미가 없게 되버림..   
분포가 많은 쪽으로 수렴  
nox의 중요도가 (가중치가) 떨어짐  
뒤바뀐 순서가 나오면 그 설명변수를 잘 보기  


ㄹ쏘 : 최소의 절대치로 줄이는 것  
norm == distance  
l1 norm 맨하탄 거리
l2 norm 유클리드  

규제르 키운마늠 0에 가까운 거리고 오게 되는 거임  
맨하탄 = 마름모 
유클리드 = 원형



ㅣ1dms a많이 키우다보면   
등고선ㅇ 0에 가까워지는 쪽으로 기울어진 모양으로 내려옫 보면
가장 먼저 선을 접하는 점이 

멀어질수록 분포가 많은 쪽으로 가는 거임  
==> 비율이 바뀔수잇음  가중치가 바뀜  
분포가 많은 쪽으로 w값을 줄여나가다 보면 점점더 영에 한없이 수렴하고 수렴할때 라인을 접하는데 그 접하는 라인이 해당 알파값을 줬을때 최적의 w값임  


결합하면
l1 l2중에 어디에 더 가중치를 둘건지'/??
엘라스틱 norm  


찌그러진 원 = 한쪽으로 치우짐  

극단적으로 갔을때 중요도가 없는 변수를 삭제하는 Featuere Selection 
정말 중요한 피쳐만 골라내는 용도  


l2 norm 은 어떻게 해도 한쪼이 작아지진 않음  
l1은 삭제 가능  


![image](https://user-images.githubusercontent.com/82145878/179431234-2fb7856d-7463-45c5-ad06-05ee2ff5ae2e.png)  
![image](https://user-images.githubusercontent.com/82145878/179431287-f88d9110-1e0b-4acc-89db-33cd7d791d5a.png)  

  
  
  
sigmoid function (Logistic regression )
0과 1로 수렴  



Decision Tree Regression  


