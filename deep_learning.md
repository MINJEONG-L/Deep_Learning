# deep learning  



w 값은 똑같이 쓰는데 
alpha 는 하이퍼 파라미터 ==> learning rate오 ㅏ다름
즉 제약의 크기라고 ㅎㄹ 수 잇음

전첵 작아지려면 alpha 값을 키우면 w는 0에 수렴할 수 밖ㅇ없음
어느정도 가다가 멈춘거나 마찬가지 실제로는 ㅇ
그러나 w가 0에 수렴한채로.. 멈축거랑 비슷
alpha 가 0이되면 규제가 없는것과 마찬가지 
그냥 rss가 최소가 되는 점을 찾는것과같음



다항회귀에서 1-10차수까지 늘리면서 학습
1차수 일때 람다를 0-5까지 바꾸면서 하이퍼파라ㅣㅁ터 ㄴ튜닝
: 편향 분산을 줄여나가면서 

w값이 주어들면서 전체적으로 평평하게.... 윙래로 움직이는 값이 줄어드들음 
 
 w가 크다는 것은 결과값에 많은 영향  
 
알팓값을 높이니까 ㄴnox 값의 순서가 바뀌었따. ==> 
nox 데이터가 잘못들어왓다
아웃라이어가 많다거나 편향된 데잍거ㅏ 많다거나
다른 계쑤와 상관관계...가 잘못잡혓거나알
nox 값이 의미가 없게 되버림..   
분포가 많은 쪽으로 수렴  
nox의 중요도가 (가중치가) 떨어짐  
뒤바뀐 순서가 나오면 그 설명변수를 잘 보기  


ㄹ쏘 : 최소의 절대치로 줄이는 것  
norm == distance  
l1 norm 맨하탄 거리
l2 norm 유클리드  

규제르 키운마늠 0에 가까운 거리고 오게 되는 거임  
맨하탄 = 마름모 
유클리드 = 원형



ㅣ1dms a많이 키우다보면   
등고선ㅇ 0에 가까워지는 쪽으로 기울어진 모양으로 내려옫 보면
가장 먼저 선을 접하는 점이 

멀어질수록 분포가 많은 쪽으로 가는 거임  
==> 비율이 바뀔수잇음  가중치가 바뀜  
분포가 많은 쪽으로 w값을 줄여나가다 보면 점점더 영에 한없이 수렴하고 수렴할때 라인을 접하는데 그 접하는 라인이 해당 알파값을 줬을때 최적의 w값임  


결합하면
l1 l2중에 어디에 더 가중치를 둘건지'/??
엘라스틱 norm  


찌그러진 원 = 한쪽으로 치우짐  

극단적으로 갔을때 중요도가 없는 변수를 삭제하는 Featuere Selection 
정말 중요한 피쳐만 골라내는 용도  


l2 norm 은 어떻게 해도 한쪼이 작아지진 않음  
l1은 삭제 가능  


![image](https://user-images.githubusercontent.com/82145878/179431234-2fb7856d-7463-45c5-ad06-05ee2ff5ae2e.png)  
![image](https://user-images.githubusercontent.com/82145878/179431287-f88d9110-1e0b-4acc-89db-33cd7d791d5a.png)  

  
  
  
sigmoid function (Logistic regression )
0과 1로 수렴  



Decision Tree Regression  
  
  
가상으 축을 새로 만들어서 여기에 투영  
축에다가 각각의 데이터를 투영 => 이 축이 최적의 축이 되고 
하나의 더 좋은 축을 설명하게도미ㅕㄴ 다중공선성해결  
어케 컴퓨터가 자동으로 찾냐? ==> 주성분분석  

차원이 4개 -> 벡터도 4개
근데 2개로 줄이고 싶음  
가상의 축 -> 데이터들이 모임  

딥러닝.... : 차원을 줄엿다 늘렷다 반복  
학습에 의한 w값으로 차즈러 가는 과정  
뭐래?  


# 신경망을 위한 데이터 표현  
1) 스칼라(0D 텐서)  
 - 넘파이에서 float32나 float64 타입의 숫자가 스칼라 텐서  
 ```python
 import numpy as np  
 x = np.array(12)  
 x 
 x.ndim
 ```  
 
2) 벡터(1D 텐서) 
 ```python
 x = np.array([12,3,6,14,7])
 x
 x.ndim
 ```  
 
3) 행렬(2D 텐서)  
 ```python
 x = np.array([[5, 78, 2, 34, 0],
             [6, 79, 3, 35, 1], 
              [7, 80, 4, 36, 2]])
 x
 x.ndim
 ```  
 
 4) 3D 텐서  
  ![image](https://user-images.githubusercontent.com/82145878/179446395-9f7ecde3-6835-42a6-a51a-380003c287ba.png)  
  
 5) 고차원 텐서  
  ![image](https://user-images.githubusercontent.com/82145878/179446487-8c6fb851-eaae-4bbd-9962-6616de11f561.png)  

 ![image](https://user-images.githubusercontent.com/82145878/179445779-a77c91e2-27d5-4792-8fd5-e47c180350ce.png)  
 
 
 
 

 
